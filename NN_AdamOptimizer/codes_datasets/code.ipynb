{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        np.random.seed(37)\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2 / (input_dim + output_dim))\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(x, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_weights = np.dot(self.input.T, d_out)\n",
    "        d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        # Store gradients for the optimizer\n",
    "        self.d_weights = d_weights\n",
    "        self.d_bias = d_bias\n",
    "        return d_input\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.input > 0)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        #print(x.shape)\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*x.shape) > self.rate) / (1 - self.rate)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        #print(params[0].weights.shape,params[0].bias.shape)\n",
    "        self.m = [(np.zeros_like(param.weights),np.zeros_like(param.bias)) for param in params] \n",
    "        self.v = [(np.zeros_like(param.weights),np.zeros_like(param.bias)) for param in params]\n",
    "        #print(self.m[1][1].shape)\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, params):\n",
    "        self.t += 1\n",
    "        #for i, param in enumerate(params):\n",
    "        #    print(i,param.weights.shape,param.bias.shape)\n",
    "        \n",
    "        for i, param in enumerate(params):\n",
    "            self.m[i]=list(self.m[i])\n",
    "            self.v[i]=list(self.v[i])\n",
    "            self.m[i][0] = self.beta1 * self.m[i][0] + (1 - self.beta1) * param.d_weights\n",
    "            self.m[i][1] = self.beta1 * self.m[i][1] + (1 - self.beta1) * param.d_bias\n",
    "\n",
    "            self.v[i][0] = self.beta2 * self.v[i][0] + (1 - self.beta2) * param.d_weights**2\n",
    "            self.v[i][1] = self.beta2 * self.v[i][1] + (1 - self.beta2) * param.d_bias**2\n",
    "            self.m[i]=tuple(self.m[i])\n",
    "            self.v[i]=tuple(self.v[i])\n",
    "\n",
    "            m_hat = self.m[i][0] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i][0] / (1 - self.beta2**self.t)\n",
    "            param.weights -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "            m_hat = self.m[i][1] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i][1] / (1 - self.beta2**self.t)\n",
    "            param.bias -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            \n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, logits, labels):\n",
    "        #print(labels.shape)\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        #exps = np.exp(logits)\n",
    "        self.probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.labels = labels\n",
    "        return -np.mean(np.sum(labels * np.log(self.probs + 1e-9), axis=1))\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.probs - self.labels) / self.labels.shape[0]\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers, learning_rate=0.001):\n",
    "        self.layers = layers\n",
    "        self.optimizer = Adam(params=[layer for layer in layers if isinstance(layer, Dense)], lr=learning_rate)\n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_out = layer.backward(d_out)\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn.forward(logits, y)\n",
    "        d_out = self.loss_fn.backward()\n",
    "        self.backward(d_out)\n",
    "        self.optimizer.update(params=[layer for layer in self.layers if isinstance(layer, Dense)])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_scores(self, x, y,val=False):\n",
    "        logits = self.forward(x)\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        #exp_logits=np.exp(logits)\n",
    "        \n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "\n",
    "        labels = y\n",
    "        if len(labels.shape) == 1:\n",
    "            num_classes = np.max(labels) + 1\n",
    "            labels = np.eye(num_classes)[labels]\n",
    "        loss = -np.mean(np.sum(labels * np.log(probs + 1e-9), axis=1))\n",
    "\n",
    "        if len(labels.shape) != 1:\n",
    "            labels=np.argmax(labels, axis=1)\n",
    "        accuracy= np.mean(predictions == labels)*100\n",
    "        \n",
    "        f1_macro = f1_score(labels, predictions, average='macro')\n",
    "        if(val):\n",
    "             return {\n",
    "            'Loss': loss,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1_macro\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "            'Loss': loss,\n",
    "            'Accuracy': accuracy\n",
    "            } \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model_weights(layers, filename=\"model_weights.pkl\"):\n",
    "    model_weights = {}\n",
    "    for i, layer in enumerate(layers):\n",
    "        model_weights[f'layer_{i}'] = {\n",
    "            'weights': layer.weights,\n",
    "            'bias': layer.bias\n",
    "        }\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model_weights, file)\n",
    "    print(f\"Model weights saved to {filename}\")\n",
    "\n",
    "def load_model_weights(layers, filename=\"model_weights.pkl\"):\n",
    "    with open(filename, 'rb') as file:\n",
    "        model_weights = pickle.load(file)\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        if f'layer_{i}' in model_weights:\n",
    "            layer.weights = model_weights[f'layer_{i}']['weights']\n",
    "            layer.bias = model_weights[f'layer_{i}']['bias']\n",
    "    print(f\"Model weights loaded from {filename}\")\n",
    "\n",
    "\n",
    "# After training your model\n",
    "#save_model_weights(layers, \"trained_model.pkl\")\n",
    "#load_model_weights(layers,\"trained_model.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fashion-mnist_train.csv')\n",
    "print(df.shape)\n",
    "y = df['label']\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]\n",
    "X = df.drop(columns=['label'])\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.1, random_state=37)\n",
    "\n",
    "df_test=pd.read_csv('fashion-mnist_test.csv')\n",
    "y_test = df_test['label']\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "X_test=scaler.transform(X_test) #-------------------------------------------IMPORTANT\n",
    "\n",
    "layers = [\n",
    "    Dense(X.shape[1], 256),\n",
    "    ReLU(),\n",
    "    Dropout(0.1),\n",
    "    Dense(256, 128),\n",
    "    ReLU(),\n",
    "    Dropout(0.1),\n",
    "    Dense(128, 32),\n",
    "    ReLU(),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, 10),\n",
    "]\n",
    "\n",
    "model = Model(layers, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to load best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlayers=[layer for layer in model.layers if isinstance(layer, Dense)]\\nload_model_weights(layers,\"trained_model.pkl\")\\n\\nmetrics_test=model.get_scores(X_test, y_test, val=True)\\n\\nprint(f\"Test Accuracy: {metrics_test[\\'Accuracy\\']:.2f}%, Test loss: {metrics_test[\\'Loss\\']}, F1 score: {metrics_test[\\'F1 Score\\']:.2f}\")\\n'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "layers=[layer for layer in model.layers if isinstance(layer, Dense)]\n",
    "load_model_weights(layers,\"trained_model.pkl\")\n",
    "\n",
    "metrics_test=model.get_scores(X_test, y_test, val=True)\n",
    "\n",
    "print(f\"Test Accuracy: {metrics_test['Accuracy']:.2f}%, Test loss: {metrics_test['Loss']}, F1 score: {metrics_test['F1 Score']:.2f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Else if you want to train a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, avg Loss: 0.5356\n",
      "Training loss: 0.41274306099637836, Training accuracy: 84.92%\n",
      "Validation loss: 0.44853934331198775, Validation accuracy: 83.95%, F1 score: 0.8385\n",
      "Epoch 2/10, avg Loss: 0.3960\n",
      "Training loss: 0.3597915720058951, Training accuracy: 86.96%\n",
      "Validation loss: 0.40084167801076764, Validation accuracy: 85.93%, F1 score: 0.8579\n",
      "Epoch 3/10, avg Loss: 0.3587\n",
      "Training loss: 0.3264731810931201, Training accuracy: 88.18%\n",
      "Validation loss: 0.37286984335779916, Validation accuracy: 86.63%, F1 score: 0.8652\n",
      "Epoch 4/10, avg Loss: 0.3312\n",
      "Training loss: 0.30859571039046435, Training accuracy: 88.77%\n",
      "Validation loss: 0.37756188439351185, Validation accuracy: 86.73%, F1 score: 0.8659\n",
      "Epoch 5/10, avg Loss: 0.3145\n",
      "Training loss: 0.28867270425162345, Training accuracy: 89.47%\n",
      "Validation loss: 0.35903098458913824, Validation accuracy: 87.62%, F1 score: 0.8747\n",
      "Epoch 6/10, avg Loss: 0.2931\n",
      "Training loss: 0.27402089866513735, Training accuracy: 89.85%\n",
      "Validation loss: 0.3450726516525373, Validation accuracy: 88.03%, F1 score: 0.8794\n",
      "Epoch 7/10, avg Loss: 0.2814\n",
      "Training loss: 0.2642367482012363, Training accuracy: 90.17%\n",
      "Validation loss: 0.3507200854840177, Validation accuracy: 87.83%, F1 score: 0.8772\n",
      "Epoch 8/10, avg Loss: 0.2655\n",
      "Training loss: 0.2559617284701966, Training accuracy: 90.67%\n",
      "Validation loss: 0.35550952091110244, Validation accuracy: 87.92%, F1 score: 0.8773\n",
      "Epoch 9/10, avg Loss: 0.2532\n",
      "Training loss: 0.2632949187903679, Training accuracy: 90.54%\n",
      "Validation loss: 0.36501656628749907, Validation accuracy: 87.83%, F1 score: 0.8767\n",
      "Epoch 10/10, avg Loss: 0.2433\n",
      "Training loss: 0.227889829906792, Training accuracy: 91.56%\n",
      "Validation loss: 0.35161326836946016, Validation accuracy: 88.58%, F1 score: 0.8842\n",
      "Test Accuracy: 88.20%, Test loss: 0.3497809967225114, F1 score: 0.8815\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "num_batches = X.shape[0] // batch_size\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        #minibatch\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X[start:end]\n",
    "        y_batch = y[start:end]\n",
    "        \n",
    "        #loss calculation\n",
    "        logits = model.forward(X_batch)\n",
    "        loss = model.loss_fn.forward(logits, y_batch)\n",
    "        \n",
    "        #full pass and optimization step\n",
    "        model.train_step(X_batch, y_batch)\n",
    "        \n",
    "        epoch_loss += loss\n",
    "    \n",
    "    #average loss for the epoch\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, avg Loss: {avg_loss:.4f}\")\n",
    "    #metrics after each epoch\n",
    "    metrics_train=model.get_scores(X, y, val=False)\n",
    "    metrics_val=model.get_scores(X_val, y_val,val=True)\n",
    "    print(f\"Training loss: {metrics_train['Loss']}, Training accuracy: {metrics_train['Accuracy']:.2f}%\")\n",
    "    print(f\"Validation loss: {metrics_val['Loss']}, Validation accuracy: {metrics_val['Accuracy']:.2f}%, F1 score: {metrics_val['F1 Score']:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Testset\n",
    "'''df_test=pd.read_csv('fashion-mnist_test.csv')\n",
    "y_test = df_test['label']\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "X_test=scaler.transform(X_test) #-------------------------------------------IMPORTANT'''\n",
    "metrics_test=model.get_scores(X_test, y_test, val=True)\n",
    "\n",
    "print(f\"Test Accuracy: {metrics_test['Accuracy']:.2f}%, Test loss: {metrics_test['Loss']}, F1 score: {metrics_test['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you want to save newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to best.pkl\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "layers=[layer for layer in model.layers if isinstance(layer, Dense)]\n",
    "save_model_weights(layers, \"best.pkl\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
